{
 "metadata": {
  "name": "",
  "signature": "sha256:ff58d3ba1bfd2b2e5f0886eb7889dfc9bb00246fde1868506d5f1fe3632d307a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "import numpy as np \n",
      "import pandas as pd\n",
      "import pandas.io.data as web\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "import re\n",
      "from sklearn.cross_validation import train_test_split, cross_val_score\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
      "import psycopg2\n",
      "import pandas.io.sql as psql\n",
      "import ipdb\n",
      "import sys\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.metrics import confusion_matrix, silhouette_score, roc_curve\n",
      "from sklearn.decomposition import NMF\n",
      "import cPickle\n",
      "import matplotlib.pyplot as plt\n",
      "from nltk.classify import NaiveBayesClassifier\n",
      "from nltk.corpus import movie_reviews, stopwords\n",
      "from pandas.tseries.offsets import *\n",
      "from dateutil import parser\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from pprint import pprint\n",
      "from time import time\n",
      "\n",
      "def generate_tfidf(data):\n",
      "    '''\n",
      "        Description:\n",
      "            Generates a tfidf vectorizer\n",
      "\n",
      "        Parameters:\n",
      "            Returns: a parameterized vectorizer\n",
      "    '''\n",
      "    # n_samples = 2000\n",
      "    n_features = 10000\n",
      "    n_topics = 10\n",
      "    n_top_words = 20\n",
      "    \n",
      "    # ipdb.set_trace()\n",
      "    # tfidf  = TfidfVectorizer(max_df=0.95, min_df=min_df, max_features=n_features, stop_words='english')\n",
      "    tfidf  = TfidfVectorizer(max_features=n_features, stop_words='english')\n",
      "    clfv = tfidf.fit_transform(data)\n",
      "\n",
      "    # with open ('kmeans_tfidf.pkl', 'wb') as fid:\n",
      "    #     cPickle.dump(tfidf, fid)\n",
      "\n",
      "    return tfidf, clfv\n",
      "\n",
      "def getConn(DBNAME, DBUSER, PASSWRD, tablename, bLabel=False):\n",
      "    '''\n",
      "        Description:\n",
      "            Generic database connection\n",
      "\n",
      "        Parameters:\n",
      "            Database name, user, password, tablename\n",
      "            Returns: dataframe with URL, title, content, date\n",
      "    '''\n",
      "   \n",
      "    conn = psycopg2.connect(database=DBNAME, user=DBUSER, password=PASSWRD)\n",
      "\n",
      "    # flexible sql statement to return labeled and unlabeled data\n",
      "    if not bLabel:\n",
      "        sql = 'SELECT url, title, content, date, label from ' + tablename + ' where label is null'\n",
      "    else:\n",
      "        sql = 'SELECT  url, title, content, date, label from ' + tablename + ' where label is NOT null'\n",
      "\n",
      "    df = psql.frame_query(sql, conn)\n",
      "    conn.close()\n",
      "\n",
      "    return df\n",
      "\n",
      "def getScrapedContent(bLabel):\n",
      "    '''\n",
      "        Description:\n",
      "            Gets data from multiple datasources having URL, title, content, date and returns a merged dataframe\n",
      "\n",
      "        Parameters:\n",
      "            Returns a merged dataframe that hasn't been cleaned\n",
      "    '''\n",
      "\n",
      "    DBNAME = zip(['newscontent', 'financenews'],['stocknews_newscontent2', 'data2'])\n",
      "    DBUSER = 'ethancheung'\n",
      "    PASSWRD = open('password.txt').readline()\n",
      "\n",
      "    rDf = pd.DataFrame()\n",
      "    for eDB in DBNAME:\n",
      "        rDf = rDf.append(getConn(eDB[0], DBUSER, PASSWRD, eDB[1], bLabel))\n",
      "    return rDf\n",
      "\n",
      "def combineHistVolColumn(contentDf, volDf):\n",
      "    '''          \n",
      "       Description:\n",
      "          Scraped content dataframe from postgres\n",
      "          Volatility dataframe from google\n",
      "\n",
      "       RETURNS:\n",
      "          Merges the scraped web content with the historical volatility labels\n",
      "          Precondition: Scraped content dataframe, volatility dataframe\n",
      "          Returns: Content dataframe, label dataframe\n",
      "    '''\n",
      "    # reset the index to make the Date column available to be joined with other dataframe\n",
      "\n",
      "    volDf = volDf.reset_index()\n",
      "\n",
      "    volDf.columns = ['index', 'Volatility']\n",
      "    #make date columns same type\n",
      "    contentDf['Date_obj'] = pd.to_datetime(contentDf['date'])\n",
      "\n",
      "    # ENHANCEMENT: calculate the volatility for each document per given time period\n",
      "\n",
      "    volDf['Date_obj'] = pd.to_datetime(volDf['index'])\n",
      "\n",
      "    merged = pd.merge(contentDf, volDf, on='Date_obj', how='outer')\n",
      "\n",
      "    # handle NAs\n",
      "    merged = merged.dropna(subset=['content', 'url'])\n",
      "    X = merged[['content', 'url']]\n",
      "\n",
      "    y = merged['Volatility']\n",
      "    # fill in the weekends with 0 volatilty\n",
      "    y = y.fillna(0)\n",
      "\n",
      "    return X, y\n",
      " \n",
      "def getHistoricalVolatility():\n",
      "    '''\n",
      "        Description:\n",
      "            Builds the S&P daily historical volatility dataframe since 2012-11-15 to end of 2014\n",
      "\n",
      "        Parameters:\n",
      "            Returns: Dataframe of S&P using 1 day lag\n",
      "    '''\n",
      "    data_start = '2012-11-15'\n",
      "    sp = web.DataReader('^GSPC', data_source='yahoo', start = data_start, end = '2014-12-31')\n",
      "\n",
      "    sp['IntraDay_Vol'] = sp['High']-sp['Low']       # intra_vol will be proxy for volatility instead of taking std of closing prices\n",
      "    sp.fillna(0, inplace=True)\n",
      "\n",
      "    time_period = 30\n",
      "\n",
      "    # calculate the volatility delta over a defined range before and after each date\n",
      "    vol_delta = pd.DataFrame()\n",
      "\n",
      "    temp_dict = {}\n",
      "    for eDate in sp.index.date:\n",
      "        temp_dict[eDate] = volatility_delta(eDate, time_period, sp)\n",
      "\n",
      "    vol_delta = vol_delta.from_dict(temp_dict, orient='index')\n",
      "    vol_delta = vol_delta.sort_index()\n",
      "\n",
      "    return vol_delta\n",
      "\n",
      "def volatility_delta(doc_date, time_period, df):\n",
      "    '''\n",
      "        Description:\n",
      "            Without exact time stamps for documents and minute financial data, assumes that documents for a given day contributes to the volatility for that day\n",
      "            (i.e. documents are not separated by day)\n",
      "        Parameters:\n",
      "            Document date and the period of interest to calculate the volatility differences\n",
      "            Data dataframe containing historical volatility\n",
      "    '''\n",
      "    #return volatility(price_data(doc_datetime - time_period)) - volatility(price_data(doc_datetime + time_period))\n",
      "    # before time frame\n",
      "\n",
      "    before = df.ix[price_data(doc_date, time_period, df, True)]['IntraDay_Vol']\n",
      "    before.fillna(0, inplace=True)\n",
      "    after = df.ix[price_data(doc_date, time_period, df, False)]['IntraDay_Vol']\n",
      "    after.fillna(0, inplace=True)\n",
      "\n",
      "    v_before = volatility(before)\n",
      "    v_after = volatility(after)\n",
      "    vol_delta = v_before - v_after\n",
      "\n",
      "    return vol_delta\n",
      "\n",
      "def price_data(obj_doc_date, time_frame, data_df, bBefore):\n",
      "    '''\n",
      "        Description:\n",
      "            Helper function for volatility_delta\n",
      "            time_frame is measured in business days \n",
      "            doc_date is the median of the date range\n",
      "\n",
      "            Returns the volatility data for the period before and after.  After period is inclusive of the document release date\n",
      "            Receives data_df and indexed by .['2014-1-30': XXX]\n",
      "\n",
      "            XXX is determined from the time_frame.  For example, 2014-1-23 if time_frame is week and 2014-2-6 after\n",
      "    '''\n",
      "\n",
      "    # make the start date\n",
      "    if type(obj_doc_date) == 'str':\n",
      "        obj_doc_date = parser.parse(obj_doc_date)\n",
      "    # ensure that beginning period does not include the day the document is released\n",
      "    if bBefore:\n",
      "        data_window = pd.date_range(obj_doc_date - (time_frame + 1) * BDay(), obj_doc_date - 1 * Day())\n",
      "    else:\n",
      "        data_window = pd.date_range(obj_doc_date, obj_doc_date + time_frame * BDay())\n",
      "    \n",
      "    # get volatility for the period before and after\n",
      "    return data_window #(data_df.ix[range_before]['intra_vol'], data_df.ix[range_after]['intra_vol'])\n",
      "\n",
      "def volatility(price_data):\n",
      "    '''\n",
      "        Description:\n",
      "            Helper function for volatility_delta\n",
      "            Dataframe containing the price data of the S&P for a specified period\n",
      "\n",
      "        Parameters:\n",
      "            Receives data for a window period\n",
      "            Returns volatility of price range data\n",
      "    '''\n",
      "    return np.std(price_data)\n",
      "\n",
      "\n",
      "def displayScore(clf, X_train, y_train, X_test, y_test, y_pred):\n",
      "    '''\n",
      "        Description:\n",
      "            Generalizable display score function\n",
      "\n",
      "        Parameters:\n",
      "            Receives various test, train dataframes\n",
      "            Returns: Nothing\n",
      "    '''\n",
      "    # 1 estimator score method                                                                     \n",
      "    print \"\\nEstimator score method: \", str(clf.score(X_test, y_test)) + '\\n'\n",
      "\n",
      "    # 2 scoring parameter   \n",
      "    try:                                                                       \n",
      "        scores = cross_val_score(clf, X_train, y_train, cv=2, scoring='accuracy')\n",
      "        print \"Scoring parameter 'accuracy' from cross val: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() / 2)\n",
      "        # 3 scoring via metric functions                                                               \n",
      "        # print average_precision_score(y_test, y_pred)                                                \n",
      "        print confusion_matrix(y_test, y_pred)\n",
      "    except:\n",
      "        # pass in the case of unevaluable sparse matrices\n",
      "        pass\n",
      "\n",
      "def calculateSentiment(X):\n",
      "    '''\n",
      "        Description:\n",
      "            Calculates sentiment as a feature of predicting volatility\n",
      "\n",
      "        Parameters:\n",
      "            Receives the corpus of documents\n",
      "            Returns dataframe of one column containing sentiment\n",
      "    '''\n",
      "    nb_classifer = train_sentiment_classifier()\n",
      "    tokenizer = RegexpTokenizer(r'\\w+')\n",
      "\n",
      "    sentDict = {}\n",
      "    for idx, eDoc in enumerate(X):\n",
      "        # classify the sentiment\n",
      "        newdict         = {}\n",
      "        for i in tokenizer.tokenize(eDoc):\n",
      "            newdict[i] = True\n",
      "        sentDict[idx] = nb_classifer.classify(newdict)\n",
      "    sentDf = pd.DataFrame.from_dict(sentDict, orient='index')\n",
      "    sentDf = sentDf.sort_index()\n",
      "    return sentDf\n",
      "\n",
      "def linear_reg():\n",
      "\n",
      "    # get content that is labeled using getScraped\n",
      "    # Case 1: for supervised learning\n",
      "    article_df                = getScrapedContent(True)\n",
      "    df1_label                 = article_df['label']\n",
      "    df1_content               = article_df[['content','date']]\n",
      "    sp_df                     = getHistoricalVolatility()\n",
      "    X, y_vol                  = combineHistVolColumn(df1_content, sp_df)\n",
      "    X_sentiment               = calculateSentiment(X)\n",
      "    # generate tfidf\n",
      "    tfidf, clfv = generate_tfidf(X['content'])\n",
      "    X_train, X_test, y_train, y_test = train_test_split(clfv, df1_label, test_size=0.4, random_state=42)\n",
      "\n",
      "    clf = LinearRegression()\n",
      "    clf.fit(X_train, y_train)\n",
      "\n",
      "    y_pred = clf.predict(X_test)\n",
      "    displayScore(clf, X_train, y_train, X_test, y_test, y_pred)\n",
      "\n",
      "def word_feats(words):\n",
      "    '''\n",
      "        Description:\n",
      "            Helper function for sentiment classifier\n",
      "\n",
      "        Parameters:\n",
      "            Receives words\n",
      "            Returns dict\n",
      "    '''\n",
      "\n",
      "    return dict([(word, True) for word in words])\n",
      "\n",
      "def train_sentiment_classifier():\n",
      "    '''\n",
      "        Description:\n",
      "            Trains naive bayes classifier \n",
      "            Bootstrapped with basic movie word corpus\n",
      "\n",
      "        Parameters:\n",
      "            Returns: a NB trained classifier\n",
      "    '''\n",
      "    negids = movie_reviews.fileids('neg')\n",
      "    posids = movie_reviews.fileids('pos')\n",
      "\n",
      "    negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
      "    posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
      " \n",
      "    negcutoff = len(negfeats)*3/4\n",
      "    poscutoff = len(posfeats)*3/4\n",
      "\n",
      "    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
      "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
      " \n",
      "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
      "    return classifier\n",
      "\n",
      "def kmeans_logistic():\n",
      "\n",
      "    # Case 2: for unsupervised/semisupervised\n",
      "    article_df2               = getScrapedContent(False)\n",
      "    df2_content               = article_df2[['content','date']]\n",
      "    df2_date                  = article_df2['date']\n",
      "    sp_df                     = getHistoricalVolatility()\n",
      "    X, y_vol                  = combineHistVolColumn(df2_content, sp_df)\n",
      "\n",
      "    # generate vectorized clfv\n",
      "    tfidf, clfv = generate_tfidf(X['content'])\n",
      "\n",
      "    clf = KMeans(n_clusters=10, init='k-means++', max_iter=100) #, n_init=1)\n",
      "    clf.fit_predict(clfv)\n",
      "    labels = clf.labels_\n",
      "\n",
      "    # with open ('kmeans_km_model.pkl', 'wb') as fid:\n",
      "    #     cPickle.dump(clf, fid)\n",
      "\n",
      "    print '\\nSilouette score :', str(silhouette_score(clfv, labels, metric='euclidean')) + '\\n'\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(clfv, labels, test_size=0.4, random_state=42)        \n",
      "\n",
      "    clf_lr = LogisticRegression()\n",
      "    clf_lr.fit(X_train, y_train)\n",
      "\n",
      "    y_pred = clf_lr.predict(X_test)\n",
      "\n",
      "#     with open ('km_lr_tfi_model.pkl', 'wb') as fid:\n",
      "#         cPickle.dump((clf_lr, tfidf), fid)\n",
      "    \n",
      "    displayScore(clf_lr, X_train, y_train, X_test, y_test, y_pred)\n",
      "\n",
      "def calculate_CrossCorrelation(df, n_topics):\n",
      "    '''\n",
      "        Description:\n",
      "            Calculates Beta (i.e. tendency) of a given 1-D series with Volatility\n",
      "            Assumes incoming dataframe has Volatility column\n",
      "\n",
      "        Parameters:\n",
      "            Receives: Dataframe\n",
      "            Returns: Numpy array\n",
      "    '''\n",
      "    sBeta = []\n",
      "    for nTopics in xrange(n_topics):\n",
      "        cov = np.cov(df[nTopics], df['Volatility'])\n",
      "        sBeta.append(cov[1, 0] / cov[0, 0])\n",
      "\n",
      "    return sBeta\n",
      "\n",
      "# def displayROC(X_test, y_test, clf):\n",
      "\n",
      "#     probas_ = clf.predict_proba(X_test)\n",
      "\n",
      "#     ipdb.set_trace()\n",
      "\n",
      "#     fpr, tpr,thresholds = roc_curve(y_test, probas_) \n",
      "#     roc_auc = auc(fpr, tpr)\n",
      "#     print(\"Area under the ROC curve : %f\" % roc_auc)\n",
      "\n",
      "#     plt.figure()\n",
      "#     plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
      "#     plt.plot([0, 1], [0, 1], 'k--')\n",
      "#     plt.xlim([0.0, 1.0])\n",
      "#     plt.ylim([0.0, 1.05])\n",
      "#     plt.xlabel('False Positive Rate')\n",
      "#     plt.ylabel('True Positive Rate')\n",
      "#     plt.title('Region operating characteristic')\n",
      "#     plt.legend(loc=\"lower right\")\n",
      "#     plt.show()\n",
      "\n",
      "def lin_regression():\n",
      "    '''\n",
      "        Description:\n",
      "            Runs a multiple regression of the topics + polarity against the dependent variable volatility\n",
      "            to determine the weights of each variable\n",
      "        Parameters:\n",
      "            Receives independent variables (topics + sentiment), dependent variable Volatility\n",
      "            returns weights of each topic\n",
      "\n",
      "    '''\n",
      "    clf = LinearRegression()\n",
      "    clf.fit_predict(X,y)\n",
      "\n",
      "\n",
      "def add_Binary_Features(orig_Content, featureDf):\n",
      "    '''\n",
      "        Description:\n",
      "            Calculates sentiment, source info, binarizes and adds to feature DataFrame\n",
      "        Parameters:\n",
      "            Receives original content dataframe, feature DataFrame\n",
      "            Returns updated dataframe with binarized features\n",
      "    '''\n",
      "    sentDf                  = pd.DataFrame(calculateSentiment(orig_Content['content']))\n",
      "    # binarize the sentiment and add one column\n",
      "    dummies1 = pd.get_dummies(sentDf[0])\n",
      "    featureDf['Sentiment'] = dummies1.iloc[:,:-1]\n",
      "\n",
      "\n",
      "    orig_Content['url'] = orig_Content['url'].apply(lambda x: x.split('.')[1])\n",
      "\n",
      "    dummies2 = pd.get_dummies(orig_Content['url'])\n",
      "    pd.concat([featureDf.head(), dummies2.iloc[:,:-1].head()], join='outer', axis=1)\n",
      "\n",
      "    featureDf['Date'] = featureDf['Date'].apply(lambda x: pd.to_datetime(x).dayofweek)\n",
      "\n",
      "    return featureDf\n",
      "\n",
      "def importance(data, label):\n",
      "    \"\"\"Compute feature importance using decision trees classifier.\n",
      "  \n",
      "      INPUT: data  -- numeric pandas dataframe with non-missing values\n",
      "             label -- boolean pandas series with which to predict on\n",
      "  \n",
      "    OUTPUT: results sent to stdout\n",
      "    \"\"\"\n",
      "\n",
      "    clf = ExtraTreesClassifier()\n",
      "    clf.fit(data, label)\n",
      "  \n",
      "    for imp, col in sorted( zip(clf.feature_importances_, data.columns), key=lambda (imp, col): imp, reverse=True ):\n",
      "        print \"[{:.5f}] {}\".format(imp, col)\n",
      "\n",
      "\n",
      "def gridsearch(X_train, y_train):\n",
      "\n",
      "    arPipeline = []\n",
      "    arParameters = []\n",
      "    arPipeline.append(Pipeline([\n",
      "                    ('logclf', LogisticRegression()),\n",
      "    ]))\n",
      "    arPipeline.append(Pipeline([\n",
      "                    ('rfclf', RandomForestClassifier()),\n",
      "    ]))\n",
      "    arParameters.append({\n",
      "        'logclf__C': (10, 1, 0.1, 0.01),\n",
      "    })\n",
      "    arParameters.append({\n",
      "        'rfclf__n_estimators': (1, 5), \n",
      "        'rfclf__min_samples_split': (2,5),\n",
      "    })\n",
      "\n",
      "    for pipeline, parameters in zip(arPipeline, arParameters):\n",
      "        print \"begin training\"\n",
      "        grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy')\n",
      "\n",
      "        print('Perfrming grid search')\n",
      "        print('pipeline:', [name for name in pipeline.steps])\n",
      "        print('parameters:') \n",
      "        pprint (parameters)\n",
      "\n",
      "        grid_search.fit(X_train, y_train)\n",
      "\n",
      "        print('Grid best score: %0.3f' % grid_search.best_score_)\n",
      "        print(\"Best parameters set:\")\n",
      "        best_parameters = grid_search.best_estimator_.get_params()\n",
      "        for param_name in sorted(parameters.keys()):\n",
      "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 343
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# X['content'][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def nmf_logistic():\n",
      "\n",
      "# Case 2: for unsupervised/semisupervised\n",
      "article_df2               = getScrapedContent(False)\n",
      "df2_content               = article_df2[['content','date', 'url']]\n",
      "df2_date                  = article_df2['date']\n",
      "sp_df                     = getHistoricalVolatility()\n",
      "X, y_vol                  = combineHistVolColumn(df2_content, sp_df)\n",
      "\n",
      "# when using nmf, X_test is used bc I want to discover latent topics\n",
      "n_topics = 10\n",
      "n_top_words = 15\n",
      "n_clf = NMF(n_components=n_topics, random_state=1)\n",
      "# this is document to topic matrix - shows percentage of each topic to each article\n",
      "# i will try to show the correlation of each topic to the volatility\n",
      "#  doc/day  |   retail sales   |    employmt   |    VOLATILIITY\n",
      "#   1             .238                .145           .05\n",
      "#   2             .123                .1             .02\n",
      "# pull in date in column and take a moving average window of one week\n",
      "tfidf, clfv = generate_tfidf(X['content'])\n",
      "W = n_clf.fit_transform(clfv)\n",
      "H = n_clf.components_\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 344
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr = pd.DataFrame(W)\n",
      "\n",
      "W_corr['Date'] = df2_date.values\n",
      "W_corr['Volatility'] = y_vol.values\n",
      "\n",
      "# add binarize features\n",
      "# data = add_Binary_Features(X, W_corr)\n",
      "\n",
      "sentDf                  = pd.DataFrame(calculateSentiment(X['content']))\n",
      "# binarize the sentiment and add one column\n",
      "X.head()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>content</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Americas economy shrank at a drastic 2.9 perce...</td>\n",
        "      <td> http://www.foxnews.com/us/2014/06/25/economy-i...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> FILE - In this March 17, 2014 file photo, Robe...</td>\n",
        "      <td> http://www.foxnews.com/us/2014/06/25/orders-fo...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> WASHINGTON \u2013  The U.S. economy shrank at a ste...</td>\n",
        "      <td> http://www.foxnews.com/us/2014/06/25/us-econom...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> DETROIT \u2013  U.N. experts say water shutoffs at ...</td>\n",
        "      <td> http://www.foxnews.com/us/2014/06/25/un-critic...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> SAN FRANCISCO (MarketWatch) -- Moodys Investor...</td>\n",
        "      <td> http://www.marketwatch.com/News/Story/Story.as...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 345,
       "text": [
        "                                             content  \\\n",
        "0  Americas economy shrank at a drastic 2.9 perce...   \n",
        "1  FILE - In this March 17, 2014 file photo, Robe...   \n",
        "2  WASHINGTON \u2013  The U.S. economy shrank at a ste...   \n",
        "3  DETROIT \u2013  U.N. experts say water shutoffs at ...   \n",
        "4  SAN FRANCISCO (MarketWatch) -- Moodys Investor...   \n",
        "\n",
        "                                                 url  \n",
        "0  http://www.foxnews.com/us/2014/06/25/economy-i...  \n",
        "1  http://www.foxnews.com/us/2014/06/25/orders-fo...  \n",
        "2  http://www.foxnews.com/us/2014/06/25/us-econom...  \n",
        "3  http://www.foxnews.com/us/2014/06/25/un-critic...  \n",
        "4  http://www.marketwatch.com/News/Story/Story.as...  "
       ]
      }
     ],
     "prompt_number": 345
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 347
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dummies1 = pd.get_dummies(sentDf[0])\n",
      "W_corr['Sentiment'] = dummies1.iloc[:,:-1]\n",
      "\n",
      "\n",
      "X['url'] = X['url'].apply(lambda x: x.split('.')[1])\n",
      "\n",
      "dummies2 = pd.get_dummies(X['url'])\n",
      "\n",
      "\n",
      "dummies2.head()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>foxnews</th>\n",
        "      <th>html</th>\n",
        "      <th>marketwatch</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 347,
       "text": [
        "   foxnews  html  marketwatch\n",
        "0        1     0            0\n",
        "1        1     0            0\n",
        "2        1     0            0\n",
        "3        1     0            0\n",
        "4        0     0            1"
       ]
      }
     ],
     "prompt_number": 347
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dummies1.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>neg</th>\n",
        "      <th>pos</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 349,
       "text": [
        "   neg  pos\n",
        "0    0    1\n",
        "1    0    1\n",
        "2    0    1\n",
        "3    0    1\n",
        "4    0    1"
       ]
      }
     ],
     "prompt_number": 349
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = W_corr['Volatility'] > 1\n",
      "y = W_corr['Volatility'] < -1\n",
      "W_corr[np.logical_or(x,y)].head()\n",
      "# set the HasVolatility = 1 if there is volatility\n",
      "W_corr['HasVolatility'] = np.zeros(W_corr.shape[0])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 287
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.where(((W_corr['Volatility'] > 1)) | ((W_corr['Volatility'] < -1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 313,
       "text": [
        "(array([ 141,  142,  143,  144,  145,  146,  148,  149,  150,  151,  152,\n",
        "         153,  154,  155,  156,  157,  158,  159,  160,  161,  162,  163,\n",
        "         164,  165,  166,  167,  168,  169,  170,  171,  172,  173,  176,\n",
        "         177,  178,  179,  182,  183,  184,  197,  198,  207,  208,  209,\n",
        "         210,  211,  229,  230,  231,  232,  233,  234,  235,  236,  237,\n",
        "         238,  241,  242,  243,  258,  535,  536,  537,  538,  539,  540,\n",
        "         541,  543,  544,  545,  547,  548,  549,  550,  551,  553,  554,\n",
        "         555,  556,  557,  558,  559,  564,  565,  566,  568,  569,  570,\n",
        "         573,  576,  731,  732,  733,  739,  740,  741,  742,  743,  744,\n",
        "         745,  746,  747,  748,  749,  750,  751,  752,  753,  754,  755,\n",
        "         759,  760,  763,  764,  765,  766,  768,  769,  771, 1034, 1035,\n",
        "        1036, 1038, 1039, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049,\n",
        "        1050, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1061, 1062,\n",
        "        1064, 1065, 1066, 1068, 1069, 1070, 1071, 1075, 1076, 1077, 1082,\n",
        "        1083, 1085, 1086, 1089, 1090, 1091, 1099, 1100, 1101, 1102, 1103,\n",
        "        1110, 1111, 1112, 1113, 1126, 1127, 1128, 1136, 1137, 1138, 1143,\n",
        "        1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1155, 1156, 1157,\n",
        "        1158, 1159, 1160, 1164, 1168, 1169, 1170, 1171, 1176, 1177, 1178,\n",
        "        1179, 1181, 1182, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1202,\n",
        "        1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1221, 1222, 1223,\n",
        "        1224, 1225, 1236, 1237, 1238, 1239, 1240, 1244, 1245, 1246, 1247,\n",
        "        1248, 1249, 1256, 1257, 1258, 1259, 1270, 1271, 1272, 1273, 1274,\n",
        "        1280, 1285, 1286, 1287, 1288, 1289, 1294, 1295, 1296, 1297, 1312,\n",
        "        1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1329, 1330,\n",
        "        1331, 1332, 1334, 1335, 1336, 1337, 1338, 1340, 1341, 1342, 1343,\n",
        "        1344, 1345, 1346, 1349, 1351, 1352, 1353, 1356, 1357, 1359, 1360,\n",
        "        1361, 1362, 1363, 1364, 1393, 1394, 1395, 1403, 1404, 1405, 1406,\n",
        "        1407, 1408, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418,\n",
        "        1419, 1420, 1423, 1424, 1430, 1431, 1432, 1433, 1434, 1435, 1436,\n",
        "        1445, 1450, 1451, 1452, 1453, 1457, 1458, 1459, 1460, 1463, 1464,\n",
        "        1465, 1479, 1480, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489,\n",
        "        1490, 1491, 1492, 1493, 1497, 1500, 1501, 1504, 1505, 1506, 1507,\n",
        "        1511, 1512, 1513, 1516, 1517, 1518, 1519, 1524, 1525, 1528, 1530,\n",
        "        1531, 1532, 1534, 1540, 1548, 1549, 1550, 1553, 1554, 1556, 1557,\n",
        "        1558, 1567, 1569, 1570, 1572, 1573, 1577, 1578, 1580, 1581, 1582,\n",
        "        1583, 1584, 1585, 1588, 1589, 1590, 1592, 1593, 1603, 1604, 1611,\n",
        "        1612, 1614, 1615]),)"
       ]
      }
     ],
     "prompt_number": 313
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr['HasVolatility'] = W_corr['Volatility'].apply(lambda x: 1 if x > 1 or x < -1 else 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 319
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr['HasVolatility'].unique()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 321,
       "text": [
        "array([0, 1])"
       ]
      }
     ],
     "prompt_number": 321
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 322
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "yVol = W_corr.pop('HasVolatility')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 323
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = W_corr.pop('Volatility')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 325
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 326
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr = pd.concat([W_corr, dummies2.iloc[:,:-1]], join='outer', axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 264
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr['Has_Volatility'] = np.where(W_corr['Volatility'] > 1 or W_corr['Volatility'] < -1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 269,
       "text": [
        "-7.8216182300131223"
       ]
      }
     ],
     "prompt_number": 269
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.where((W_corr['Volatility'] > 1) | ([W_corr['Volatility'] < -1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-281-c03b09332df2>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-281-c03b09332df2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    np.where((W_corr['Volatility'] > 1) | ([W_corr['Volatility'] < -1))\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 281
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr['Date'] = W_corr['Date'].apply(lambda x: pd.to_datetime(x).dayofweek)\n",
      "    \n",
      "label = data.pop('Volatility')\n",
      "\n",
      "# determine feature importances\n",
      "importance(data, label)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'str' object has no attribute 'pop'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-266-aab3a66893b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mW_corr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_corr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Volatility'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# determine feature importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'pop'"
       ]
      }
     ],
     "prompt_number": 266
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.bar(df2_date, label.values, width=0.7, edgecolor='none', color='r', \n",
      "        label=\"Volatility per Time Period\",\n",
      "        )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "<Container object of 3114 artists>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAL2CAYAAABlkBO1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEgQUwAACWIKACBBTAEAJPw/HG8INFgP1rcAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1135256d0>"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#     model  = LogisticRegression(C=1, penalty='l1', tol=0.01)\n",
      "#     scores = cross_val_score(model, data, label, cv=5)\n",
      "#     print \"%s -- %s\" % (model.__class__, np.mean(scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "textdata = \"Amazon.com (AMZN) shares getting hammered today after last night's earnings report. While revenue was broadly in-line with expectations, profit missed the mark badly as Jeff Bezos continues to show no regard for profitability.The details Amazon reported a second quarter net loss of $126 million, or 27 cents per share, while the street expected a loss of around 15 cents a share. Operating expenses for the quarter came in at $19.36 billion.Net sales were reported to be $19.34 billion, up 23% from 2013 second quarter\u2019s figure of $15.7 billion. Revenue closely matched analyst expectations, and basically within the company\u2019s sales guidance range of $18.1 billion and $19.8 billion.Wait\u2026 What?!It was a fairly normal quarter right up until the company suggested it would lose between $410 and $810 million in the current quarter. It\u2019s one thing to not sweat profits but quite another to idly project a potential four-fifths of a billion loss with no real explanation. The instant the guidance was issued Amazon shares dropped a quick 5% more after-hours. Nothing that was said over the balance of the call lifted investors\u2019 skepticism.\u201cA lot of the earlier expansions were in to similar businesses or digital versions of physical businesses but they\u2019re getting so far afield now,\u201d grouses Aaron Pressman in the attached video. \u201cPhones is a very competitive business and it seems like their phone is going to do nothing. The cloud business was a very good business but there\u2019s been huge price cutting.\u201dInvestors have been giving Amazon a pass on profitability for as long as the company has been public. When Jeff Bezos filed to go public way back in 1997, he said right up front that he wouldn\u2019t be running the company to make short term profits. \u201cThe Company believes that it will incur substantial operating losses for the foreseeable future, and that the rate at which such losses will be incurred will increase significant from current levels,\u201d Bezos wrote in the SEC filing.It may be the most honest statement ever written in a government filing. True to his word Bezos and Amazon have racked up losses and shareholders have largely forgiven them for doing so. Still, there\u2019s a limit. The magnitude and vagary of the projected losses for the current quarter were a bridge too far as far as shareholders were concerned.Last night was a nearly verbatim repeat of what Amazon said in 1997 but not even Bezos can get the benefit of the doubt when projecting a nearly billion loss without explaining where the money is going.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 210
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "textdata = ''.join([i if ord(i) < 128 else ' ' for i in text])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "orig_Content = pd.DataFrame([[textdata,'2014-7-25','www.foxnews.com/blah.aspx']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 240,
       "text": [
        "TfidfVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "        vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 240
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clfv = tfidf.transform([textdata])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "orig_Content.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Amazon.com (AMZN) shares getting hammered toda...</td>\n",
        "      <td> 2014-7-25</td>\n",
        "      <td> www.foxnews.com/blah.aspx</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 242,
       "text": [
        "                                                   0          1  \\\n",
        "0  Amazon.com (AMZN) shares getting hammered toda...  2014-7-25   \n",
        "\n",
        "                           2  \n",
        "0  www.foxnews.com/blah.aspx  "
       ]
      }
     ],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "orig_Content.columns = ['content','Date','url']\n",
      "orig_Content.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>content</th>\n",
        "      <th>Date</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Amazon.com (AMZN) shares getting hammered toda...</td>\n",
        "      <td> 2014-7-25</td>\n",
        "      <td> www.foxnews.com/blah.aspx</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 243,
       "text": [
        "                                             content       Date  \\\n",
        "0  Amazon.com (AMZN) shares getting hammered toda...  2014-7-25   \n",
        "\n",
        "                         url  \n",
        "0  www.foxnews.com/blah.aspx  "
       ]
      }
     ],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clfv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 244,
       "text": [
        "<1x10000 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 144 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_clf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 245,
       "text": [
        "NMF(beta=1, eta=0.1, init=None, max_iter=200, n_components=10,\n",
        "  nls_max_iter=2000, random_state=1, sparseness=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W = n_clf.transform(clfv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 255,
       "text": [
        "(1, 10)"
       ]
      }
     ],
     "prompt_number": 255
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 256,
       "text": [
        "array([[-0.        , -0.        , -0.        , -0.        ,  0.0042616 ,\n",
        "         0.04060652, -0.        , -0.        ,  0.15171959,  0.00205986]])"
       ]
      }
     ],
     "prompt_number": 256
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "      <th>Date</th>\n",
        "      <th>Sentiment</th>\n",
        "      <th>foxnews</th>\n",
        "      <th>html</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.00000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.001124</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.048285</td>\n",
        "      <td> 0.012772</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.035046</td>\n",
        "      <td> 0.171156</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0.001481</td>\n",
        "      <td>-0.00000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.015122</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.012551</td>\n",
        "      <td> 0.014820</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.008359</td>\n",
        "      <td> 0.192143</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.00000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.031111</td>\n",
        "      <td> 0.155289</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.00000</td>\n",
        "      <td> 0.005655</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.000493</td>\n",
        "      <td> 0.036034</td>\n",
        "      <td> 0.024491</td>\n",
        "      <td> 0.003666</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.004318</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.00183</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td>-0.000000</td>\n",
        "      <td> 0.045772</td>\n",
        "      <td> 0.008087</td>\n",
        "      <td> 0.002069</td>\n",
        "      <td> 0.019670</td>\n",
        "      <td> 0.011094</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 257,
       "text": [
        "          0        1         2         3         4         5         6  \\\n",
        "0 -0.000000 -0.00000 -0.000000  0.001124 -0.000000  0.048285  0.012772   \n",
        "1  0.001481 -0.00000 -0.000000  0.015122 -0.000000  0.012551  0.014820   \n",
        "2 -0.000000 -0.00000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000   \n",
        "3 -0.000000 -0.00000  0.005655 -0.000000  0.000493  0.036034  0.024491   \n",
        "4 -0.000000  0.00183 -0.000000 -0.000000 -0.000000  0.045772  0.008087   \n",
        "\n",
        "          7         8         9  Date  Sentiment  foxnews  html  \n",
        "0 -0.000000  0.035046  0.171156     2          0        1     0  \n",
        "1 -0.000000  0.008359  0.192143     0          0        1     0  \n",
        "2 -0.000000  0.031111  0.155289     2          0        1     0  \n",
        "3  0.003666 -0.000000  0.004318     0          0        1     0  \n",
        "4  0.002069  0.019670  0.011094     1          0        0     0  "
       ]
      }
     ],
     "prompt_number": 257
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train the linear regression to predict volatility"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = LinearRegression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = W_corr.pop('Volatility')\n",
      "W_corr.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'Volatility'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-259-1b55f32ad21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_corr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Volatility'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mW_corr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdrop\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mRaise\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1678\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionaility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   2563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2565\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2566\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2567\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0mloc\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munique\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly\u001b[0m \u001b[0mslice\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \"\"\"\n\u001b[0;32m-> 1181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/index.so\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3354)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/index.so\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3234)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/hashtable.so\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:11148)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/hashtable.so\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:11101)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mKeyError\u001b[0m: 'Volatility'"
       ]
      }
     ],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(W_corr,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 179,
       "text": [
        "LinearRegression(copy_X=True, fit_intercept=True, normalize=False)"
       ]
      }
     ],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(clf.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 184,
       "text": [
        "14"
       ]
      }
     ],
     "prompt_number": 184
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 250,
       "text": [
        "(1, 10)"
       ]
      }
     ],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "orig_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>content</th>\n",
        "      <th>Date</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Amazon.com (AMZN) shares getting hammered toda...</td>\n",
        "      <td> 2014-7-25</td>\n",
        "      <td> www.foxnews.com/blah.aspx</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 251,
       "text": [
        "                                             content       Date  \\\n",
        "0  Amazon.com (AMZN) shares getting hammered toda...  2014-7-25   \n",
        "\n",
        "                         url  \n",
        "0  www.foxnews.com/blah.aspx  "
       ]
      }
     ],
     "prompt_number": 251
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 251
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    sentDf                  = pd.DataFrame(calculateSentiment(orig_df['content']))\n",
      "    # binarize the sentiment and add one column\n",
      "    dummies1 = pd.get_dummies(sentDf[0])\n",
      "    dummies1.columns = ['Sentiment'] \n",
      "    W = pd.concat([pd.DataFrame(W), dummies1], join='outer', axis =1)\n",
      "\n",
      "    orig_Content['url'] = orig_Content['url'].apply(lambda x: x.split('.')[1])\n",
      "\n",
      "    dummies2 = pd.get_dummies(orig_Content['url'])\n",
      "    pd.concat([W, dummies2.iloc[:,:-1]], join='outer', axis=1)\n",
      "\n",
      "    W['Date'] = W['Date'].apply(lambda x: pd.to_datetime(x).dayofweek)\n",
      "\n",
      "sentDf.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "list index out of range",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-253-c62e92c1db28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummies1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0morig_Content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_Content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdummies2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_Content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/lib.so\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:51482)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m<ipython-input-253-c62e92c1db28>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummies1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0morig_Content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_Content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdummies2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_Content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: list index out of range"
       ]
      }
     ],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "dummies1 = pd.get_dummies(sentDf[0])\n",
      "dummies1.columns = ['Sentiment']\n",
      "\n",
      "# W['Sentiment'] = dummies1.iloc[:,:-1]\n",
      "W = pd.concat([pd.DataFrame(W), dummies1], join='outer', axis =1)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 237
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "      <th>pos</th>\n",
        "      <th>Sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>-0</td>\n",
        "      <td>-0</td>\n",
        "      <td>-0</td>\n",
        "      <td>-0</td>\n",
        "      <td> 0.004262</td>\n",
        "      <td> 0.040607</td>\n",
        "      <td>-0</td>\n",
        "      <td>-0</td>\n",
        "      <td> 0.15172</td>\n",
        "      <td> 0.00206</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 238,
       "text": [
        "   0  1  2  3         4         5  6  7        8        9  pos  Sentiment\n",
        "0 -0 -0 -0 -0  0.004262  0.040607 -0 -0  0.15172  0.00206    1          1"
       ]
      }
     ],
     "prompt_number": 238
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "    orig_Content['url'] = orig_Content['url'].apply(lambda x: x.split('.')[1])\n",
      "\n",
      "    dummies2 = pd.get_dummies(orig_Content['url'])\n",
      "    pd.concat([featureDf.head(), dummies2.iloc[:,:-1].head()], join='outer', axis=1)\n",
      "\n",
      "    featureDf['Date'] = featureDf['Date'].apply(lambda x: pd.to_datetime(x).dayofweek)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "unexpected indent (<ipython-input-327-3642250b37e1>, line 2)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-327-3642250b37e1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    orig_Content['url'] = orig_Content['url'].apply(lambda x: x.split('.')[1])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
       ]
      }
     ],
     "prompt_number": 327
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sp = pd.read_csv('hist_vol.csv',index_col='Date', parse_dates=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 338
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sp.index.date"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 341,
       "text": [
        "array([datetime.date(2012, 11, 15), datetime.date(2012, 11, 16),\n",
        "       datetime.date(2012, 11, 19), datetime.date(2012, 11, 20),\n",
        "       datetime.date(2012, 11, 21), datetime.date(2012, 11, 23),\n",
        "       datetime.date(2012, 11, 26), datetime.date(2012, 11, 27),\n",
        "       datetime.date(2012, 11, 28), datetime.date(2012, 11, 29),\n",
        "       datetime.date(2012, 11, 30), datetime.date(2012, 12, 3),\n",
        "       datetime.date(2012, 12, 4), datetime.date(2012, 12, 5),\n",
        "       datetime.date(2012, 12, 6), datetime.date(2012, 12, 7),\n",
        "       datetime.date(2012, 12, 10), datetime.date(2012, 12, 11),\n",
        "       datetime.date(2012, 12, 12), datetime.date(2012, 12, 13),\n",
        "       datetime.date(2012, 12, 14), datetime.date(2012, 12, 17),\n",
        "       datetime.date(2012, 12, 18), datetime.date(2012, 12, 19),\n",
        "       datetime.date(2012, 12, 20), datetime.date(2012, 12, 21),\n",
        "       datetime.date(2012, 12, 24), datetime.date(2012, 12, 26),\n",
        "       datetime.date(2012, 12, 27), datetime.date(2012, 12, 28),\n",
        "       datetime.date(2012, 12, 31), datetime.date(2013, 1, 2),\n",
        "       datetime.date(2013, 1, 3), datetime.date(2013, 1, 4),\n",
        "       datetime.date(2013, 1, 7), datetime.date(2013, 1, 8),\n",
        "       datetime.date(2013, 1, 9), datetime.date(2013, 1, 10),\n",
        "       datetime.date(2013, 1, 11), datetime.date(2013, 1, 14),\n",
        "       datetime.date(2013, 1, 15), datetime.date(2013, 1, 16),\n",
        "       datetime.date(2013, 1, 17), datetime.date(2013, 1, 18),\n",
        "       datetime.date(2013, 1, 22), datetime.date(2013, 1, 23),\n",
        "       datetime.date(2013, 1, 24), datetime.date(2013, 1, 25),\n",
        "       datetime.date(2013, 1, 28), datetime.date(2013, 1, 29),\n",
        "       datetime.date(2013, 1, 30), datetime.date(2013, 1, 31),\n",
        "       datetime.date(2013, 2, 1), datetime.date(2013, 2, 4),\n",
        "       datetime.date(2013, 2, 5), datetime.date(2013, 2, 6),\n",
        "       datetime.date(2013, 2, 7), datetime.date(2013, 2, 8),\n",
        "       datetime.date(2013, 2, 11), datetime.date(2013, 2, 12),\n",
        "       datetime.date(2013, 2, 13), datetime.date(2013, 2, 14),\n",
        "       datetime.date(2013, 2, 15), datetime.date(2013, 2, 19),\n",
        "       datetime.date(2013, 2, 20), datetime.date(2013, 2, 21),\n",
        "       datetime.date(2013, 2, 22), datetime.date(2013, 2, 25),\n",
        "       datetime.date(2013, 2, 26), datetime.date(2013, 2, 27),\n",
        "       datetime.date(2013, 2, 28), datetime.date(2013, 3, 1),\n",
        "       datetime.date(2013, 3, 4), datetime.date(2013, 3, 5),\n",
        "       datetime.date(2013, 3, 6), datetime.date(2013, 3, 7),\n",
        "       datetime.date(2013, 3, 8), datetime.date(2013, 3, 11),\n",
        "       datetime.date(2013, 3, 12), datetime.date(2013, 3, 13),\n",
        "       datetime.date(2013, 3, 14), datetime.date(2013, 3, 15),\n",
        "       datetime.date(2013, 3, 18), datetime.date(2013, 3, 19),\n",
        "       datetime.date(2013, 3, 20), datetime.date(2013, 3, 21),\n",
        "       datetime.date(2013, 3, 22), datetime.date(2013, 3, 25),\n",
        "       datetime.date(2013, 3, 26), datetime.date(2013, 3, 27),\n",
        "       datetime.date(2013, 3, 28), datetime.date(2013, 4, 1),\n",
        "       datetime.date(2013, 4, 2), datetime.date(2013, 4, 3),\n",
        "       datetime.date(2013, 4, 4), datetime.date(2013, 4, 5),\n",
        "       datetime.date(2013, 4, 8), datetime.date(2013, 4, 9),\n",
        "       datetime.date(2013, 4, 10), datetime.date(2013, 4, 11),\n",
        "       datetime.date(2013, 4, 12), datetime.date(2013, 4, 15),\n",
        "       datetime.date(2013, 4, 16), datetime.date(2013, 4, 17),\n",
        "       datetime.date(2013, 4, 18), datetime.date(2013, 4, 19),\n",
        "       datetime.date(2013, 4, 22), datetime.date(2013, 4, 23),\n",
        "       datetime.date(2013, 4, 24), datetime.date(2013, 4, 25),\n",
        "       datetime.date(2013, 4, 26), datetime.date(2013, 4, 29),\n",
        "       datetime.date(2013, 4, 30), datetime.date(2013, 5, 1),\n",
        "       datetime.date(2013, 5, 2), datetime.date(2013, 5, 3),\n",
        "       datetime.date(2013, 5, 6), datetime.date(2013, 5, 7),\n",
        "       datetime.date(2013, 5, 8), datetime.date(2013, 5, 9),\n",
        "       datetime.date(2013, 5, 10), datetime.date(2013, 5, 13),\n",
        "       datetime.date(2013, 5, 14), datetime.date(2013, 5, 15),\n",
        "       datetime.date(2013, 5, 16), datetime.date(2013, 5, 17),\n",
        "       datetime.date(2013, 5, 20), datetime.date(2013, 5, 21),\n",
        "       datetime.date(2013, 5, 22), datetime.date(2013, 5, 23),\n",
        "       datetime.date(2013, 5, 24), datetime.date(2013, 5, 28),\n",
        "       datetime.date(2013, 5, 29), datetime.date(2013, 5, 30),\n",
        "       datetime.date(2013, 5, 31), datetime.date(2013, 6, 3),\n",
        "       datetime.date(2013, 6, 4), datetime.date(2013, 6, 5),\n",
        "       datetime.date(2013, 6, 6), datetime.date(2013, 6, 7),\n",
        "       datetime.date(2013, 6, 10), datetime.date(2013, 6, 11),\n",
        "       datetime.date(2013, 6, 12), datetime.date(2013, 6, 13),\n",
        "       datetime.date(2013, 6, 14), datetime.date(2013, 6, 17),\n",
        "       datetime.date(2013, 6, 18), datetime.date(2013, 6, 19),\n",
        "       datetime.date(2013, 6, 20), datetime.date(2013, 6, 21),\n",
        "       datetime.date(2013, 6, 24), datetime.date(2013, 6, 25),\n",
        "       datetime.date(2013, 6, 26), datetime.date(2013, 6, 27),\n",
        "       datetime.date(2013, 6, 28), datetime.date(2013, 7, 1),\n",
        "       datetime.date(2013, 7, 2), datetime.date(2013, 7, 3),\n",
        "       datetime.date(2013, 7, 5), datetime.date(2013, 7, 8),\n",
        "       datetime.date(2013, 7, 9), datetime.date(2013, 7, 10),\n",
        "       datetime.date(2013, 7, 11), datetime.date(2013, 7, 12),\n",
        "       datetime.date(2013, 7, 15), datetime.date(2013, 7, 16),\n",
        "       datetime.date(2013, 7, 17), datetime.date(2013, 7, 18),\n",
        "       datetime.date(2013, 7, 19), datetime.date(2013, 7, 22),\n",
        "       datetime.date(2013, 7, 23), datetime.date(2013, 7, 24),\n",
        "       datetime.date(2013, 7, 25), datetime.date(2013, 7, 26),\n",
        "       datetime.date(2013, 7, 29), datetime.date(2013, 7, 30),\n",
        "       datetime.date(2013, 7, 31), datetime.date(2013, 8, 1),\n",
        "       datetime.date(2013, 8, 2), datetime.date(2013, 8, 5),\n",
        "       datetime.date(2013, 8, 6), datetime.date(2013, 8, 7),\n",
        "       datetime.date(2013, 8, 8), datetime.date(2013, 8, 9),\n",
        "       datetime.date(2013, 8, 12), datetime.date(2013, 8, 13),\n",
        "       datetime.date(2013, 8, 14), datetime.date(2013, 8, 15),\n",
        "       datetime.date(2013, 8, 16), datetime.date(2013, 8, 19),\n",
        "       datetime.date(2013, 8, 20), datetime.date(2013, 8, 21),\n",
        "       datetime.date(2013, 8, 22), datetime.date(2013, 8, 23),\n",
        "       datetime.date(2013, 8, 26), datetime.date(2013, 8, 27),\n",
        "       datetime.date(2013, 8, 28), datetime.date(2013, 8, 29),\n",
        "       datetime.date(2013, 8, 30), datetime.date(2013, 9, 3),\n",
        "       datetime.date(2013, 9, 4), datetime.date(2013, 9, 5),\n",
        "       datetime.date(2013, 9, 6), datetime.date(2013, 9, 9),\n",
        "       datetime.date(2013, 9, 10), datetime.date(2013, 9, 11),\n",
        "       datetime.date(2013, 9, 12), datetime.date(2013, 9, 13),\n",
        "       datetime.date(2013, 9, 16), datetime.date(2013, 9, 17),\n",
        "       datetime.date(2013, 9, 18), datetime.date(2013, 9, 19),\n",
        "       datetime.date(2013, 9, 20), datetime.date(2013, 9, 23),\n",
        "       datetime.date(2013, 9, 24), datetime.date(2013, 9, 25),\n",
        "       datetime.date(2013, 9, 26), datetime.date(2013, 9, 27),\n",
        "       datetime.date(2013, 9, 30), datetime.date(2013, 10, 1),\n",
        "       datetime.date(2013, 10, 2), datetime.date(2013, 10, 3),\n",
        "       datetime.date(2013, 10, 4), datetime.date(2013, 10, 7),\n",
        "       datetime.date(2013, 10, 8), datetime.date(2013, 10, 9),\n",
        "       datetime.date(2013, 10, 10), datetime.date(2013, 10, 11),\n",
        "       datetime.date(2013, 10, 14), datetime.date(2013, 10, 15),\n",
        "       datetime.date(2013, 10, 16), datetime.date(2013, 10, 17),\n",
        "       datetime.date(2013, 10, 18), datetime.date(2013, 10, 21),\n",
        "       datetime.date(2013, 10, 22), datetime.date(2013, 10, 23),\n",
        "       datetime.date(2013, 10, 24), datetime.date(2013, 10, 25),\n",
        "       datetime.date(2013, 10, 28), datetime.date(2013, 10, 29),\n",
        "       datetime.date(2013, 10, 30), datetime.date(2013, 10, 31),\n",
        "       datetime.date(2013, 11, 1), datetime.date(2013, 11, 4),\n",
        "       datetime.date(2013, 11, 5), datetime.date(2013, 11, 6),\n",
        "       datetime.date(2013, 11, 7), datetime.date(2013, 11, 8),\n",
        "       datetime.date(2013, 11, 11), datetime.date(2013, 11, 12),\n",
        "       datetime.date(2013, 11, 13), datetime.date(2013, 11, 14),\n",
        "       datetime.date(2013, 11, 15), datetime.date(2013, 11, 18),\n",
        "       datetime.date(2013, 11, 19), datetime.date(2013, 11, 20),\n",
        "       datetime.date(2013, 11, 21), datetime.date(2013, 11, 22),\n",
        "       datetime.date(2013, 11, 25), datetime.date(2013, 11, 26),\n",
        "       datetime.date(2013, 11, 27), datetime.date(2013, 11, 29),\n",
        "       datetime.date(2013, 12, 2), datetime.date(2013, 12, 3),\n",
        "       datetime.date(2013, 12, 4), datetime.date(2013, 12, 5),\n",
        "       datetime.date(2013, 12, 6), datetime.date(2013, 12, 9),\n",
        "       datetime.date(2013, 12, 10), datetime.date(2013, 12, 11),\n",
        "       datetime.date(2013, 12, 12), datetime.date(2013, 12, 13),\n",
        "       datetime.date(2013, 12, 16), datetime.date(2013, 12, 17),\n",
        "       datetime.date(2013, 12, 18), datetime.date(2013, 12, 19),\n",
        "       datetime.date(2013, 12, 20), datetime.date(2013, 12, 23),\n",
        "       datetime.date(2013, 12, 24), datetime.date(2013, 12, 26),\n",
        "       datetime.date(2013, 12, 27), datetime.date(2013, 12, 30),\n",
        "       datetime.date(2013, 12, 31), datetime.date(2014, 1, 2),\n",
        "       datetime.date(2014, 1, 3), datetime.date(2014, 1, 6),\n",
        "       datetime.date(2014, 1, 7), datetime.date(2014, 1, 8),\n",
        "       datetime.date(2014, 1, 9), datetime.date(2014, 1, 10),\n",
        "       datetime.date(2014, 1, 13), datetime.date(2014, 1, 14),\n",
        "       datetime.date(2014, 1, 15), datetime.date(2014, 1, 16),\n",
        "       datetime.date(2014, 1, 17), datetime.date(2014, 1, 21),\n",
        "       datetime.date(2014, 1, 22), datetime.date(2014, 1, 23),\n",
        "       datetime.date(2014, 1, 24), datetime.date(2014, 1, 27),\n",
        "       datetime.date(2014, 1, 28), datetime.date(2014, 1, 29),\n",
        "       datetime.date(2014, 1, 30), datetime.date(2014, 1, 31),\n",
        "       datetime.date(2014, 2, 3), datetime.date(2014, 2, 4),\n",
        "       datetime.date(2014, 2, 5), datetime.date(2014, 2, 6),\n",
        "       datetime.date(2014, 2, 7), datetime.date(2014, 2, 10),\n",
        "       datetime.date(2014, 2, 11), datetime.date(2014, 2, 12),\n",
        "       datetime.date(2014, 2, 13), datetime.date(2014, 2, 14),\n",
        "       datetime.date(2014, 2, 18), datetime.date(2014, 2, 19),\n",
        "       datetime.date(2014, 2, 20), datetime.date(2014, 2, 21),\n",
        "       datetime.date(2014, 2, 24), datetime.date(2014, 2, 25),\n",
        "       datetime.date(2014, 2, 26), datetime.date(2014, 2, 27),\n",
        "       datetime.date(2014, 2, 28), datetime.date(2014, 3, 3),\n",
        "       datetime.date(2014, 3, 4), datetime.date(2014, 3, 5),\n",
        "       datetime.date(2014, 3, 6), datetime.date(2014, 3, 7),\n",
        "       datetime.date(2014, 3, 10), datetime.date(2014, 3, 11),\n",
        "       datetime.date(2014, 3, 12), datetime.date(2014, 3, 13),\n",
        "       datetime.date(2014, 3, 14), datetime.date(2014, 3, 17),\n",
        "       datetime.date(2014, 3, 18), datetime.date(2014, 3, 19),\n",
        "       datetime.date(2014, 3, 20), datetime.date(2014, 3, 21),\n",
        "       datetime.date(2014, 3, 24), datetime.date(2014, 3, 25),\n",
        "       datetime.date(2014, 3, 26), datetime.date(2014, 3, 27),\n",
        "       datetime.date(2014, 3, 28), datetime.date(2014, 3, 31),\n",
        "       datetime.date(2014, 4, 1), datetime.date(2014, 4, 2),\n",
        "       datetime.date(2014, 4, 3), datetime.date(2014, 4, 4),\n",
        "       datetime.date(2014, 4, 7), datetime.date(2014, 4, 8),\n",
        "       datetime.date(2014, 4, 9), datetime.date(2014, 4, 10),\n",
        "       datetime.date(2014, 4, 11), datetime.date(2014, 4, 14),\n",
        "       datetime.date(2014, 4, 15), datetime.date(2014, 4, 16),\n",
        "       datetime.date(2014, 4, 17), datetime.date(2014, 4, 21),\n",
        "       datetime.date(2014, 4, 22), datetime.date(2014, 4, 23),\n",
        "       datetime.date(2014, 4, 24), datetime.date(2014, 4, 25),\n",
        "       datetime.date(2014, 4, 28), datetime.date(2014, 4, 29),\n",
        "       datetime.date(2014, 4, 30), datetime.date(2014, 5, 1),\n",
        "       datetime.date(2014, 5, 2), datetime.date(2014, 5, 5),\n",
        "       datetime.date(2014, 5, 6), datetime.date(2014, 5, 7),\n",
        "       datetime.date(2014, 5, 8), datetime.date(2014, 5, 9),\n",
        "       datetime.date(2014, 5, 12), datetime.date(2014, 5, 13),\n",
        "       datetime.date(2014, 5, 14), datetime.date(2014, 5, 15),\n",
        "       datetime.date(2014, 5, 16), datetime.date(2014, 5, 19),\n",
        "       datetime.date(2014, 5, 20), datetime.date(2014, 5, 21),\n",
        "       datetime.date(2014, 5, 22), datetime.date(2014, 5, 23),\n",
        "       datetime.date(2014, 5, 27), datetime.date(2014, 5, 28),\n",
        "       datetime.date(2014, 5, 29), datetime.date(2014, 5, 30),\n",
        "       datetime.date(2014, 6, 2), datetime.date(2014, 6, 3),\n",
        "       datetime.date(2014, 6, 4), datetime.date(2014, 6, 5),\n",
        "       datetime.date(2014, 6, 6), datetime.date(2014, 6, 9),\n",
        "       datetime.date(2014, 6, 10), datetime.date(2014, 6, 11),\n",
        "       datetime.date(2014, 6, 12), datetime.date(2014, 6, 13),\n",
        "       datetime.date(2014, 6, 16), datetime.date(2014, 6, 17),\n",
        "       datetime.date(2014, 6, 18), datetime.date(2014, 6, 19),\n",
        "       datetime.date(2014, 6, 20), datetime.date(2014, 6, 23),\n",
        "       datetime.date(2014, 6, 24), datetime.date(2014, 6, 25),\n",
        "       datetime.date(2014, 6, 26), datetime.date(2014, 6, 27),\n",
        "       datetime.date(2014, 6, 30), datetime.date(2014, 7, 1),\n",
        "       datetime.date(2014, 7, 2), datetime.date(2014, 7, 3),\n",
        "       datetime.date(2014, 7, 7), datetime.date(2014, 7, 8),\n",
        "       datetime.date(2014, 7, 9), datetime.date(2014, 7, 10),\n",
        "       datetime.date(2014, 7, 11), datetime.date(2014, 7, 14),\n",
        "       datetime.date(2014, 7, 15), datetime.date(2014, 7, 16),\n",
        "       datetime.date(2014, 7, 17), datetime.date(2014, 7, 18),\n",
        "       datetime.date(2014, 7, 21), datetime.date(2014, 7, 22),\n",
        "       datetime.date(2014, 7, 23), datetime.date(2014, 7, 24),\n",
        "       datetime.date(2014, 7, 25)], dtype=object)"
       ]
      }
     ],
     "prompt_number": 341
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 341
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_start = '2012-11-15'\n",
      "sp = web.DataReader('^GSPC', data_source='yahoo', start = data_start, end = '2014-12-31')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 332
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sp.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Open</th>\n",
        "      <th>High</th>\n",
        "      <th>Low</th>\n",
        "      <th>Close</th>\n",
        "      <th>Volume</th>\n",
        "      <th>Adj Close</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Date</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>2012-11-15</th>\n",
        "      <td> 1355.41</td>\n",
        "      <td> 1360.62</td>\n",
        "      <td> 1348.05</td>\n",
        "      <td> 1353.33</td>\n",
        "      <td> 3928870000</td>\n",
        "      <td> 1353.33</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2012-11-16</th>\n",
        "      <td> 1353.36</td>\n",
        "      <td> 1362.03</td>\n",
        "      <td> 1343.35</td>\n",
        "      <td> 1359.88</td>\n",
        "      <td> 4045910000</td>\n",
        "      <td> 1359.88</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2012-11-19</th>\n",
        "      <td> 1359.88</td>\n",
        "      <td> 1386.89</td>\n",
        "      <td> 1359.88</td>\n",
        "      <td> 1386.89</td>\n",
        "      <td> 3374800000</td>\n",
        "      <td> 1386.89</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2012-11-20</th>\n",
        "      <td> 1386.82</td>\n",
        "      <td> 1389.77</td>\n",
        "      <td> 1377.04</td>\n",
        "      <td> 1387.81</td>\n",
        "      <td> 3207160000</td>\n",
        "      <td> 1387.81</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2012-11-21</th>\n",
        "      <td> 1387.79</td>\n",
        "      <td> 1391.25</td>\n",
        "      <td> 1386.39</td>\n",
        "      <td> 1391.03</td>\n",
        "      <td> 2667090000</td>\n",
        "      <td> 1391.03</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 333,
       "text": [
        "               Open     High      Low    Close      Volume  Adj Close\n",
        "Date                                                                 \n",
        "2012-11-15  1355.41  1360.62  1348.05  1353.33  3928870000    1353.33\n",
        "2012-11-16  1353.36  1362.03  1343.35  1359.88  4045910000    1359.88\n",
        "2012-11-19  1359.88  1386.89  1359.88  1386.89  3374800000    1386.89\n",
        "2012-11-20  1386.82  1389.77  1377.04  1387.81  3207160000    1387.81\n",
        "2012-11-21  1387.79  1391.25  1386.39  1391.03  2667090000    1391.03"
       ]
      }
     ],
     "prompt_number": 333
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sp.index.date"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 336,
       "text": [
        "array([datetime.date(2012, 11, 15), datetime.date(2012, 11, 16),\n",
        "       datetime.date(2012, 11, 19), datetime.date(2012, 11, 20),\n",
        "       datetime.date(2012, 11, 21), datetime.date(2012, 11, 23),\n",
        "       datetime.date(2012, 11, 26), datetime.date(2012, 11, 27),\n",
        "       datetime.date(2012, 11, 28), datetime.date(2012, 11, 29),\n",
        "       datetime.date(2012, 11, 30), datetime.date(2012, 12, 3),\n",
        "       datetime.date(2012, 12, 4), datetime.date(2012, 12, 5),\n",
        "       datetime.date(2012, 12, 6), datetime.date(2012, 12, 7),\n",
        "       datetime.date(2012, 12, 10), datetime.date(2012, 12, 11),\n",
        "       datetime.date(2012, 12, 12), datetime.date(2012, 12, 13),\n",
        "       datetime.date(2012, 12, 14), datetime.date(2012, 12, 17),\n",
        "       datetime.date(2012, 12, 18), datetime.date(2012, 12, 19),\n",
        "       datetime.date(2012, 12, 20), datetime.date(2012, 12, 21),\n",
        "       datetime.date(2012, 12, 24), datetime.date(2012, 12, 26),\n",
        "       datetime.date(2012, 12, 27), datetime.date(2012, 12, 28),\n",
        "       datetime.date(2012, 12, 31), datetime.date(2013, 1, 2),\n",
        "       datetime.date(2013, 1, 3), datetime.date(2013, 1, 4),\n",
        "       datetime.date(2013, 1, 7), datetime.date(2013, 1, 8),\n",
        "       datetime.date(2013, 1, 9), datetime.date(2013, 1, 10),\n",
        "       datetime.date(2013, 1, 11), datetime.date(2013, 1, 14),\n",
        "       datetime.date(2013, 1, 15), datetime.date(2013, 1, 16),\n",
        "       datetime.date(2013, 1, 17), datetime.date(2013, 1, 18),\n",
        "       datetime.date(2013, 1, 22), datetime.date(2013, 1, 23),\n",
        "       datetime.date(2013, 1, 24), datetime.date(2013, 1, 25),\n",
        "       datetime.date(2013, 1, 28), datetime.date(2013, 1, 29),\n",
        "       datetime.date(2013, 1, 30), datetime.date(2013, 1, 31),\n",
        "       datetime.date(2013, 2, 1), datetime.date(2013, 2, 4),\n",
        "       datetime.date(2013, 2, 5), datetime.date(2013, 2, 6),\n",
        "       datetime.date(2013, 2, 7), datetime.date(2013, 2, 8),\n",
        "       datetime.date(2013, 2, 11), datetime.date(2013, 2, 12),\n",
        "       datetime.date(2013, 2, 13), datetime.date(2013, 2, 14),\n",
        "       datetime.date(2013, 2, 15), datetime.date(2013, 2, 19),\n",
        "       datetime.date(2013, 2, 20), datetime.date(2013, 2, 21),\n",
        "       datetime.date(2013, 2, 22), datetime.date(2013, 2, 25),\n",
        "       datetime.date(2013, 2, 26), datetime.date(2013, 2, 27),\n",
        "       datetime.date(2013, 2, 28), datetime.date(2013, 3, 1),\n",
        "       datetime.date(2013, 3, 4), datetime.date(2013, 3, 5),\n",
        "       datetime.date(2013, 3, 6), datetime.date(2013, 3, 7),\n",
        "       datetime.date(2013, 3, 8), datetime.date(2013, 3, 11),\n",
        "       datetime.date(2013, 3, 12), datetime.date(2013, 3, 13),\n",
        "       datetime.date(2013, 3, 14), datetime.date(2013, 3, 15),\n",
        "       datetime.date(2013, 3, 18), datetime.date(2013, 3, 19),\n",
        "       datetime.date(2013, 3, 20), datetime.date(2013, 3, 21),\n",
        "       datetime.date(2013, 3, 22), datetime.date(2013, 3, 25),\n",
        "       datetime.date(2013, 3, 26), datetime.date(2013, 3, 27),\n",
        "       datetime.date(2013, 3, 28), datetime.date(2013, 4, 1),\n",
        "       datetime.date(2013, 4, 2), datetime.date(2013, 4, 3),\n",
        "       datetime.date(2013, 4, 4), datetime.date(2013, 4, 5),\n",
        "       datetime.date(2013, 4, 8), datetime.date(2013, 4, 9),\n",
        "       datetime.date(2013, 4, 10), datetime.date(2013, 4, 11),\n",
        "       datetime.date(2013, 4, 12), datetime.date(2013, 4, 15),\n",
        "       datetime.date(2013, 4, 16), datetime.date(2013, 4, 17),\n",
        "       datetime.date(2013, 4, 18), datetime.date(2013, 4, 19),\n",
        "       datetime.date(2013, 4, 22), datetime.date(2013, 4, 23),\n",
        "       datetime.date(2013, 4, 24), datetime.date(2013, 4, 25),\n",
        "       datetime.date(2013, 4, 26), datetime.date(2013, 4, 29),\n",
        "       datetime.date(2013, 4, 30), datetime.date(2013, 5, 1),\n",
        "       datetime.date(2013, 5, 2), datetime.date(2013, 5, 3),\n",
        "       datetime.date(2013, 5, 6), datetime.date(2013, 5, 7),\n",
        "       datetime.date(2013, 5, 8), datetime.date(2013, 5, 9),\n",
        "       datetime.date(2013, 5, 10), datetime.date(2013, 5, 13),\n",
        "       datetime.date(2013, 5, 14), datetime.date(2013, 5, 15),\n",
        "       datetime.date(2013, 5, 16), datetime.date(2013, 5, 17),\n",
        "       datetime.date(2013, 5, 20), datetime.date(2013, 5, 21),\n",
        "       datetime.date(2013, 5, 22), datetime.date(2013, 5, 23),\n",
        "       datetime.date(2013, 5, 24), datetime.date(2013, 5, 28),\n",
        "       datetime.date(2013, 5, 29), datetime.date(2013, 5, 30),\n",
        "       datetime.date(2013, 5, 31), datetime.date(2013, 6, 3),\n",
        "       datetime.date(2013, 6, 4), datetime.date(2013, 6, 5),\n",
        "       datetime.date(2013, 6, 6), datetime.date(2013, 6, 7),\n",
        "       datetime.date(2013, 6, 10), datetime.date(2013, 6, 11),\n",
        "       datetime.date(2013, 6, 12), datetime.date(2013, 6, 13),\n",
        "       datetime.date(2013, 6, 14), datetime.date(2013, 6, 17),\n",
        "       datetime.date(2013, 6, 18), datetime.date(2013, 6, 19),\n",
        "       datetime.date(2013, 6, 20), datetime.date(2013, 6, 21),\n",
        "       datetime.date(2013, 6, 24), datetime.date(2013, 6, 25),\n",
        "       datetime.date(2013, 6, 26), datetime.date(2013, 6, 27),\n",
        "       datetime.date(2013, 6, 28), datetime.date(2013, 7, 1),\n",
        "       datetime.date(2013, 7, 2), datetime.date(2013, 7, 3),\n",
        "       datetime.date(2013, 7, 5), datetime.date(2013, 7, 8),\n",
        "       datetime.date(2013, 7, 9), datetime.date(2013, 7, 10),\n",
        "       datetime.date(2013, 7, 11), datetime.date(2013, 7, 12),\n",
        "       datetime.date(2013, 7, 15), datetime.date(2013, 7, 16),\n",
        "       datetime.date(2013, 7, 17), datetime.date(2013, 7, 18),\n",
        "       datetime.date(2013, 7, 19), datetime.date(2013, 7, 22),\n",
        "       datetime.date(2013, 7, 23), datetime.date(2013, 7, 24),\n",
        "       datetime.date(2013, 7, 25), datetime.date(2013, 7, 26),\n",
        "       datetime.date(2013, 7, 29), datetime.date(2013, 7, 30),\n",
        "       datetime.date(2013, 7, 31), datetime.date(2013, 8, 1),\n",
        "       datetime.date(2013, 8, 2), datetime.date(2013, 8, 5),\n",
        "       datetime.date(2013, 8, 6), datetime.date(2013, 8, 7),\n",
        "       datetime.date(2013, 8, 8), datetime.date(2013, 8, 9),\n",
        "       datetime.date(2013, 8, 12), datetime.date(2013, 8, 13),\n",
        "       datetime.date(2013, 8, 14), datetime.date(2013, 8, 15),\n",
        "       datetime.date(2013, 8, 16), datetime.date(2013, 8, 19),\n",
        "       datetime.date(2013, 8, 20), datetime.date(2013, 8, 21),\n",
        "       datetime.date(2013, 8, 22), datetime.date(2013, 8, 23),\n",
        "       datetime.date(2013, 8, 26), datetime.date(2013, 8, 27),\n",
        "       datetime.date(2013, 8, 28), datetime.date(2013, 8, 29),\n",
        "       datetime.date(2013, 8, 30), datetime.date(2013, 9, 3),\n",
        "       datetime.date(2013, 9, 4), datetime.date(2013, 9, 5),\n",
        "       datetime.date(2013, 9, 6), datetime.date(2013, 9, 9),\n",
        "       datetime.date(2013, 9, 10), datetime.date(2013, 9, 11),\n",
        "       datetime.date(2013, 9, 12), datetime.date(2013, 9, 13),\n",
        "       datetime.date(2013, 9, 16), datetime.date(2013, 9, 17),\n",
        "       datetime.date(2013, 9, 18), datetime.date(2013, 9, 19),\n",
        "       datetime.date(2013, 9, 20), datetime.date(2013, 9, 23),\n",
        "       datetime.date(2013, 9, 24), datetime.date(2013, 9, 25),\n",
        "       datetime.date(2013, 9, 26), datetime.date(2013, 9, 27),\n",
        "       datetime.date(2013, 9, 30), datetime.date(2013, 10, 1),\n",
        "       datetime.date(2013, 10, 2), datetime.date(2013, 10, 3),\n",
        "       datetime.date(2013, 10, 4), datetime.date(2013, 10, 7),\n",
        "       datetime.date(2013, 10, 8), datetime.date(2013, 10, 9),\n",
        "       datetime.date(2013, 10, 10), datetime.date(2013, 10, 11),\n",
        "       datetime.date(2013, 10, 14), datetime.date(2013, 10, 15),\n",
        "       datetime.date(2013, 10, 16), datetime.date(2013, 10, 17),\n",
        "       datetime.date(2013, 10, 18), datetime.date(2013, 10, 21),\n",
        "       datetime.date(2013, 10, 22), datetime.date(2013, 10, 23),\n",
        "       datetime.date(2013, 10, 24), datetime.date(2013, 10, 25),\n",
        "       datetime.date(2013, 10, 28), datetime.date(2013, 10, 29),\n",
        "       datetime.date(2013, 10, 30), datetime.date(2013, 10, 31),\n",
        "       datetime.date(2013, 11, 1), datetime.date(2013, 11, 4),\n",
        "       datetime.date(2013, 11, 5), datetime.date(2013, 11, 6),\n",
        "       datetime.date(2013, 11, 7), datetime.date(2013, 11, 8),\n",
        "       datetime.date(2013, 11, 11), datetime.date(2013, 11, 12),\n",
        "       datetime.date(2013, 11, 13), datetime.date(2013, 11, 14),\n",
        "       datetime.date(2013, 11, 15), datetime.date(2013, 11, 18),\n",
        "       datetime.date(2013, 11, 19), datetime.date(2013, 11, 20),\n",
        "       datetime.date(2013, 11, 21), datetime.date(2013, 11, 22),\n",
        "       datetime.date(2013, 11, 25), datetime.date(2013, 11, 26),\n",
        "       datetime.date(2013, 11, 27), datetime.date(2013, 11, 29),\n",
        "       datetime.date(2013, 12, 2), datetime.date(2013, 12, 3),\n",
        "       datetime.date(2013, 12, 4), datetime.date(2013, 12, 5),\n",
        "       datetime.date(2013, 12, 6), datetime.date(2013, 12, 9),\n",
        "       datetime.date(2013, 12, 10), datetime.date(2013, 12, 11),\n",
        "       datetime.date(2013, 12, 12), datetime.date(2013, 12, 13),\n",
        "       datetime.date(2013, 12, 16), datetime.date(2013, 12, 17),\n",
        "       datetime.date(2013, 12, 18), datetime.date(2013, 12, 19),\n",
        "       datetime.date(2013, 12, 20), datetime.date(2013, 12, 23),\n",
        "       datetime.date(2013, 12, 24), datetime.date(2013, 12, 26),\n",
        "       datetime.date(2013, 12, 27), datetime.date(2013, 12, 30),\n",
        "       datetime.date(2013, 12, 31), datetime.date(2014, 1, 2),\n",
        "       datetime.date(2014, 1, 3), datetime.date(2014, 1, 6),\n",
        "       datetime.date(2014, 1, 7), datetime.date(2014, 1, 8),\n",
        "       datetime.date(2014, 1, 9), datetime.date(2014, 1, 10),\n",
        "       datetime.date(2014, 1, 13), datetime.date(2014, 1, 14),\n",
        "       datetime.date(2014, 1, 15), datetime.date(2014, 1, 16),\n",
        "       datetime.date(2014, 1, 17), datetime.date(2014, 1, 21),\n",
        "       datetime.date(2014, 1, 22), datetime.date(2014, 1, 23),\n",
        "       datetime.date(2014, 1, 24), datetime.date(2014, 1, 27),\n",
        "       datetime.date(2014, 1, 28), datetime.date(2014, 1, 29),\n",
        "       datetime.date(2014, 1, 30), datetime.date(2014, 1, 31),\n",
        "       datetime.date(2014, 2, 3), datetime.date(2014, 2, 4),\n",
        "       datetime.date(2014, 2, 5), datetime.date(2014, 2, 6),\n",
        "       datetime.date(2014, 2, 7), datetime.date(2014, 2, 10),\n",
        "       datetime.date(2014, 2, 11), datetime.date(2014, 2, 12),\n",
        "       datetime.date(2014, 2, 13), datetime.date(2014, 2, 14),\n",
        "       datetime.date(2014, 2, 18), datetime.date(2014, 2, 19),\n",
        "       datetime.date(2014, 2, 20), datetime.date(2014, 2, 21),\n",
        "       datetime.date(2014, 2, 24), datetime.date(2014, 2, 25),\n",
        "       datetime.date(2014, 2, 26), datetime.date(2014, 2, 27),\n",
        "       datetime.date(2014, 2, 28), datetime.date(2014, 3, 3),\n",
        "       datetime.date(2014, 3, 4), datetime.date(2014, 3, 5),\n",
        "       datetime.date(2014, 3, 6), datetime.date(2014, 3, 7),\n",
        "       datetime.date(2014, 3, 10), datetime.date(2014, 3, 11),\n",
        "       datetime.date(2014, 3, 12), datetime.date(2014, 3, 13),\n",
        "       datetime.date(2014, 3, 14), datetime.date(2014, 3, 17),\n",
        "       datetime.date(2014, 3, 18), datetime.date(2014, 3, 19),\n",
        "       datetime.date(2014, 3, 20), datetime.date(2014, 3, 21),\n",
        "       datetime.date(2014, 3, 24), datetime.date(2014, 3, 25),\n",
        "       datetime.date(2014, 3, 26), datetime.date(2014, 3, 27),\n",
        "       datetime.date(2014, 3, 28), datetime.date(2014, 3, 31),\n",
        "       datetime.date(2014, 4, 1), datetime.date(2014, 4, 2),\n",
        "       datetime.date(2014, 4, 3), datetime.date(2014, 4, 4),\n",
        "       datetime.date(2014, 4, 7), datetime.date(2014, 4, 8),\n",
        "       datetime.date(2014, 4, 9), datetime.date(2014, 4, 10),\n",
        "       datetime.date(2014, 4, 11), datetime.date(2014, 4, 14),\n",
        "       datetime.date(2014, 4, 15), datetime.date(2014, 4, 16),\n",
        "       datetime.date(2014, 4, 17), datetime.date(2014, 4, 21),\n",
        "       datetime.date(2014, 4, 22), datetime.date(2014, 4, 23),\n",
        "       datetime.date(2014, 4, 24), datetime.date(2014, 4, 25),\n",
        "       datetime.date(2014, 4, 28), datetime.date(2014, 4, 29),\n",
        "       datetime.date(2014, 4, 30), datetime.date(2014, 5, 1),\n",
        "       datetime.date(2014, 5, 2), datetime.date(2014, 5, 5),\n",
        "       datetime.date(2014, 5, 6), datetime.date(2014, 5, 7),\n",
        "       datetime.date(2014, 5, 8), datetime.date(2014, 5, 9),\n",
        "       datetime.date(2014, 5, 12), datetime.date(2014, 5, 13),\n",
        "       datetime.date(2014, 5, 14), datetime.date(2014, 5, 15),\n",
        "       datetime.date(2014, 5, 16), datetime.date(2014, 5, 19),\n",
        "       datetime.date(2014, 5, 20), datetime.date(2014, 5, 21),\n",
        "       datetime.date(2014, 5, 22), datetime.date(2014, 5, 23),\n",
        "       datetime.date(2014, 5, 27), datetime.date(2014, 5, 28),\n",
        "       datetime.date(2014, 5, 29), datetime.date(2014, 5, 30),\n",
        "       datetime.date(2014, 6, 2), datetime.date(2014, 6, 3),\n",
        "       datetime.date(2014, 6, 4), datetime.date(2014, 6, 5),\n",
        "       datetime.date(2014, 6, 6), datetime.date(2014, 6, 9),\n",
        "       datetime.date(2014, 6, 10), datetime.date(2014, 6, 11),\n",
        "       datetime.date(2014, 6, 12), datetime.date(2014, 6, 13),\n",
        "       datetime.date(2014, 6, 16), datetime.date(2014, 6, 17),\n",
        "       datetime.date(2014, 6, 18), datetime.date(2014, 6, 19),\n",
        "       datetime.date(2014, 6, 20), datetime.date(2014, 6, 23),\n",
        "       datetime.date(2014, 6, 24), datetime.date(2014, 6, 25),\n",
        "       datetime.date(2014, 6, 26), datetime.date(2014, 6, 27),\n",
        "       datetime.date(2014, 6, 30), datetime.date(2014, 7, 1),\n",
        "       datetime.date(2014, 7, 2), datetime.date(2014, 7, 3),\n",
        "       datetime.date(2014, 7, 7), datetime.date(2014, 7, 8),\n",
        "       datetime.date(2014, 7, 9), datetime.date(2014, 7, 10),\n",
        "       datetime.date(2014, 7, 11), datetime.date(2014, 7, 14),\n",
        "       datetime.date(2014, 7, 15), datetime.date(2014, 7, 16),\n",
        "       datetime.date(2014, 7, 17), datetime.date(2014, 7, 18),\n",
        "       datetime.date(2014, 7, 21), datetime.date(2014, 7, 22),\n",
        "       datetime.date(2014, 7, 23), datetime.date(2014, 7, 24),\n",
        "       datetime.date(2014, 7, 25)], dtype=object)"
       ]
      }
     ],
     "prompt_number": 336
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresDf.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'featuresDf' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-342-24c815ee5149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeaturesDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'featuresDf' is not defined"
       ]
      }
     ],
     "prompt_number": 342
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open ('/Users/ethancheung/Documents/zipfianacademy/FoxScraper/lr_lin_n_clf.pkl', 'rb') as fid:\n",
      "    n_clf, lin_clf, lr_clf, rf_clf, tfidf, nb_classifer = cPickle.load(fid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 375
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content = [\"By Lauren Tara LaCapraRelated StoriesGoldman mortgage deal with federal agency could reach $1.25 bln - source Reuters[$$] Goldman Faces Record Payout in Mortgage Case The Wall Street JournalGoldman, HSBC Count on Supreme-Court Help on Mortgage Case The Wall Street JournalGoldman Sachs shares up on earnings beat MarketWatchCOLUMN- Did this trigger Goldman's MBS 'big short'?: Frankel ReutersNEW YORK (Reuters) - A deal to resolve a U.S. regulator's claims against Goldman Sachs Group Inc over mortgage-backed securities sold to Fannie Mae and Freddie Mac leading up to the financial crisis could cost the bank between $800 million and $1.25 billion, according to a person familiar with the matter.The person said Goldman Sachs is discussing a settlement with the Federal Housing Finance Agency (FHFA), which filed 18 lawsuits against Goldman and other banks in 2011 over about $200 billion in mortgage-backed securities that later went sour.Goldman Sachs and the FHFA declined to comment on Saturday.The upper end of the range matches the amount Morgan Stanley agreed to pay in February to resolve the FHFA's claims against it. The person familiar with the matter said the negotiations are still ongoing, and the final amount of any deal remains fluid. The person was not authorized to speak publicly on the matter.The Wall Street Journal first reported the settlement talks.The FHFA has recovered $16.1 billion in agreements with other banks. Goldman is among four banks still facing FHFA mortgage-related lawsuits, along with HSBC Holdings PLC, Nomura Holdings Inc and Royal Bank of Scotland Group PLC.On Wednesday, Goldman, HSBC and Noumra argued that U.S. District Judge Denise Cote in New York, who is overseeing the litigation, should reconsider her decision that the agency did not wait too long in suing the banks.The renewed bid to dismiss the lawsuits based on timeliness issues stemmed from a June ruling from the U.S. Supreme Court. The court ruled in an environmental case that a federal law did not preempt a state-law statute that placed time limits on bringing a lawsuit that applied even if a plaintiff did not know it had a claim.But Cote warned the banks on Wednesday they faced a \\\"steep hill to climb.\\\"Goldman Sachs and HSBC are scheduled to face trial Sept. 29. A trial in the Nomura case is due for Jan. 26, 2015.The case is Federal Housing Finance Agency v. HSBC North America Holdings Inc, U.S. District Court, Southern District of New York, No. 11-6189.(Reporting by Lauren Tara LaCapra; Additional reporting by Nate Raymond; Writing by Joseph Ax; Editing by Bernard Orr)\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 384
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clfv = tfidf.transform(content)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 385
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clfv.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 387,
       "text": [
        "(1, 10000)"
       ]
      }
     ],
     "prompt_number": 387
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W = n_clf.transform(clfv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 388
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 389,
       "text": [
        "(1, 10)"
       ]
      }
     ],
     "prompt_number": 389
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr = pd.DataFrame(W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 390
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.argsort(W_corr)[::-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 4</td>\n",
        "      <td> 6</td>\n",
        "      <td> 7</td>\n",
        "      <td> 9</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td> 8</td>\n",
        "      <td> 2</td>\n",
        "      <td> 5</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 396,
       "text": [
        "   0  1  2  3  4  5  6  7  8  9\n",
        "0  4  6  7  9  1  0  3  8  2  5"
       ]
      }
     ],
     "prompt_number": 396
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_corr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0.002</td>\n",
        "      <td> 0.00121</td>\n",
        "      <td> 0.029636</td>\n",
        "      <td> 0.007115</td>\n",
        "      <td>-0</td>\n",
        "      <td> 0.053245</td>\n",
        "      <td>-0</td>\n",
        "      <td>-0</td>\n",
        "      <td> 0.023171</td>\n",
        "      <td>-0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 395,
       "text": [
        "       0        1         2         3  4         5  6  7         8  9\n",
        "0  0.002  0.00121  0.029636  0.007115 -0  0.053245 -0 -0  0.023171 -0"
       ]
      }
     ],
     "prompt_number": 395
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}